# ä»£ç è´¨é‡æ”¹è¿›è¡ŒåŠ¨è®¡åˆ’

**ç‰ˆæœ¬**: 1.0
**æ—¥æœŸ**: 2025-11-19
**åŸºäº**: é¡¹ç›®ç»ç†ä»£ç å®¡æŸ¥æŠ¥å‘Š
**ç›®æ ‡**: ç³»ç»Ÿæ€§æå‡ä»£ç è´¨é‡ã€å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

**å‘ç°é—®é¢˜**: å…±è®¡ **30+** ä¸ªé—®é¢˜
- **ä¸¥é‡ (CRITICAL)**: 4ä¸ª
- **é«˜ä¼˜å…ˆçº§ (HIGH)**: 7ä¸ª
- **ä¸­ç­‰ä¼˜å…ˆçº§ (MEDIUM)**: 11ä¸ª
- **ä½ä¼˜å…ˆçº§ (LOW)**: 8ä¸ª

**é¢„è®¡å·¥ä½œé‡**: 4-6å‘¨
**æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡**: ä» 15% â†’ 85%
**ä»£ç è´¨é‡ç›®æ ‡**: ä» 2.3/5 â†’ 4.2/5

---

## ğŸš¨ ç¬¬ä¸€é˜¶æ®µï¼šç´§æ€¥ä¿®å¤ï¼ˆWeek 1-2ï¼‰

### 1.1 å¼‚å¸¸å¤„ç†ç³»ç»Ÿæ€§é‡æ„ (CRITICAL)

**é—®é¢˜æè¿°**:
- å¤§é‡ä½¿ç”¨æ³›åŒ– `Exception` æ•è·
- ç¼ºå°‘å¼‚å¸¸é“¾ï¼ˆ`from e`ï¼‰
- åº“ä»£ç ä¸­ä½¿ç”¨ `sys.exit()`

**å½±å“èŒƒå›´**:
- `litrx/csv_analyzer.py:113-114, 149-150`
- `litrx/abstract_screener.py:126-137`
- `litrx/ai_client.py:125-149`

#### ğŸ“ è¯¦ç»†ä¿®æ”¹æ–¹æ¡ˆ

**Step 1: åˆ›å»ºå¼‚å¸¸å±‚æ¬¡ç»“æ„ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰**

```python
# litrx/exceptions.py - ç¡®ä¿åŒ…å«ä»¥ä¸‹å¼‚å¸¸ç±»

class LitRxError(Exception):
    """Base exception for all LitRx errors."""
    pass

class FileProcessingError(LitRxError):
    """Errors related to file I/O and processing."""
    pass

class APIError(LitRxError):
    """Errors related to AI API calls."""
    pass

class ConfigurationError(LitRxError):
    """Errors related to configuration."""
    def __init__(self, message: str, help_text: str = None):
        super().__init__(message)
        self.help_text = help_text

class ValidationError(LitRxError):
    """Errors related to data validation."""
    pass
```

**Step 2: ä¿®æ”¹ csv_analyzer.py**

```python
# ä¿®æ”¹å‰ (litrx/csv_analyzer.py:113-114)
except Exception as e:
    raise Exception(f"Failed to read CSV file: {str(e)}")

# âœ… ä¿®æ”¹å
from .exceptions import FileProcessingError

except FileNotFoundError as e:
    raise FileProcessingError(
        f"CSVæ–‡ä»¶æœªæ‰¾åˆ°: {file_path}"
    ) from e
except pd.errors.ParserError as e:
    raise FileProcessingError(
        f"CSVæ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚é”™è¯¯è¯¦æƒ…: {str(e)}"
    ) from e
except pd.errors.EmptyDataError as e:
    raise FileProcessingError(
        f"CSVæ–‡ä»¶ä¸ºç©º: {file_path}"
    ) from e
except Exception as e:
    logger.error(f"è¯»å–CSVæ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}", exc_info=True)
    raise FileProcessingError(
        f"æ— æ³•è¯»å–CSVæ–‡ä»¶: {str(e)}"
    ) from e
```

**Step 3: ä¿®æ”¹ abstract_screener.py ç§»é™¤ sys.exit()**

```python
# ä¿®æ”¹å‰ (litrx/abstract_screener.py:126-137)
def get_file_path_from_config(config):
    file_path = config['INPUT_FILE_PATH']
    if not file_path or file_path == 'your_input_file.xlsx':
        logger.error("é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶è·¯å¾„æœªåœ¨CONFIGä¸­æ­£ç¡®é…ç½®ã€‚")
        sys.exit(1)  # âŒ ä¸è¦åœ¨åº“ä»£ç ä¸­é€€å‡ºç¨‹åº

# âœ… ä¿®æ”¹å
from .exceptions import ConfigurationError

def get_file_path_from_config(config):
    file_path = config.get('INPUT_FILE_PATH', '')

    if not file_path or file_path == 'your_input_file.xlsx':
        raise ConfigurationError(
            "è¾“å…¥æ–‡ä»¶è·¯å¾„æœªé…ç½®",
            help_text=(
                "è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€é…ç½® INPUT_FILE_PATHï¼š\n"
                "1. åœ¨GUIä¸­é€‰æ‹©æ–‡ä»¶\n"
                "2. åœ¨ configs/config.yaml ä¸­è®¾ç½®\n"
                "3. åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ ï¼šINPUT_FILE_PATH=/path/to/file.csv\n\n"
                "ç¤ºä¾‹é…ç½®ï¼š\n"
                "INPUT_FILE_PATH: /home/user/papers.csv"
            )
        )

    return file_path
```

**Step 4: æ”¹è¿› ai_client.py çš„å¼‚å¸¸å¤„ç†**

```python
# litrx/ai_client.py:125-149

# ä¿®æ”¹å‰ï¼šå¤æ‚çš„å¼‚å¸¸é‡è¯•é€»è¾‘
except Exception as e:
    msg = str(e)
    if "param":  # âŒ è¿™ä¸ªæ¡ä»¶æ€»æ˜¯True
        pass
    if ("temperature" in kwargs) and (...):
        # é‡è¯•é€»è¾‘

# âœ… ä¿®æ”¹å
from .exceptions import APIError

except Exception as e:
    error_msg = str(e).lower()

    # ç‰¹å®šå¤„ç†ï¼štemperatureå‚æ•°ä¸è¢«æ”¯æŒ
    if "temperature" in kwargs and (
        "unsupported" in error_msg or
        "param" in error_msg and "temperature" in error_msg
    ):
        logger.warning(
            f"Model {self.model} rejected 'temperature' parameter, retrying without it"
        )
        try:
            retry_kwargs = dict(sanitized)
            retry_kwargs.pop("temperature", None)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                timeout=self.config.get("AI_TIMEOUT_SECONDS", 60),
                **retry_kwargs
            )
            logger.info("Retry succeeded without temperature parameter")
            return response.model_dump()
        except Exception as e2:
            logger.error(f"AI request failed after retry: {e2}", exc_info=True)
            raise APIError(
                f"AIè¯·æ±‚å¤±è´¥ï¼Œå³ä½¿é‡è¯•ä¹Ÿå¤±è´¥äº†ã€‚é”™è¯¯: {str(e2)}"
            ) from e2

    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
    logger.error(f"AI request failed: {e}", exc_info=True)
    raise APIError(
        f"AIè¯·æ±‚å¤±è´¥: {str(e)}\n\n"
        f"æ¨¡å‹: {self.model}\n"
        f"è¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥"
    ) from e
```

**Step 5: GUIå¼‚å¸¸å¤„ç†**

```python
# åœ¨æ‰€æœ‰GUIæ ‡ç­¾é¡µçš„ä¸»æ“ä½œæ–¹æ³•ä¸­æ·»åŠ é¡¶å±‚å¼‚å¸¸å¤„ç†

# ç¤ºä¾‹: litrx/gui/tabs_qt/csv_tab.py
def start_analysis(self):
    """å¼€å§‹CSVåˆ†æï¼ˆåœ¨åå°çº¿ç¨‹ä¸­ï¼‰"""
    try:
        # éªŒè¯è¾“å…¥
        if not self.df_file_path:
            raise ValidationError("è¯·å…ˆé€‰æ‹©CSVæ–‡ä»¶")

        # å¯åŠ¨åå°ä»»åŠ¡
        self._start_background_analysis()

    except ConfigurationError as e:
        QMessageBox.critical(
            self,
            "é…ç½®é”™è¯¯",
            f"{str(e)}\n\n{e.help_text if hasattr(e, 'help_text') else ''}"
        )
    except ValidationError as e:
        QMessageBox.warning(self, "éªŒè¯å¤±è´¥", str(e))
    except Exception as e:
        logger.error(f"å¯åŠ¨åˆ†æå¤±è´¥: {e}", exc_info=True)
        QMessageBox.critical(
            self,
            "æœªçŸ¥é”™è¯¯",
            f"å¯åŠ¨åˆ†ææ—¶å‘ç”Ÿé”™è¯¯:\n{str(e)}\n\n"
            f"è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ï¼š\n"
            f"~/.litrx/logs/litrx.log"
        )
```

**æµ‹è¯•æ¸…å•**:
```bash
# æµ‹è¯•å„ç§å¼‚å¸¸è·¯å¾„
pytest tests/test_exceptions.py -v

# æµ‹è¯•ç‚¹ï¼š
# 1. FileNotFoundError â†’ FileProcessingError
# 2. é…ç½®ç¼ºå¤± â†’ ConfigurationErrorï¼ˆå¸¦help_textï¼‰
# 3. APIé”™è¯¯ â†’ APIError
# 4. sys.exitä¸å†è¢«è°ƒç”¨
# 5. GUIæ˜¾ç¤ºå‹å¥½é”™è¯¯æ¶ˆæ¯
```

---

### 1.2 APIå¯†é’¥å®‰å…¨è¿‡æ»¤ (CRITICAL)

**é—®é¢˜æè¿°**:
- æ—¥å¿—å¯èƒ½åŒ…å«APIå¯†é’¥
- é”™è¯¯tracebackå¯èƒ½æš´éœ²æ•æ„Ÿä¿¡æ¯
- é…ç½®æ‰“å°æ²¡æœ‰è„±æ•

**å½±å“èŒƒå›´**:
- `litrx/ai_client.py:40-48`
- æ‰€æœ‰ä½¿ç”¨loggerçš„åœ°æ–¹

#### ğŸ“ è¯¦ç»†ä¿®æ”¹æ–¹æ¡ˆ

**Step 1: åˆ›å»ºå®‰å…¨æ—¥å¿—å·¥å…·**

```python
# litrx/security_utils.py (æ–°å»ºæ–‡ä»¶)

"""Security utilities for handling sensitive data."""

import re
from typing import Dict, Any

class SecureLogger:
    """å·¥å…·ç±»ï¼šå®‰å…¨åœ°è®°å½•åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ•°æ®"""

    # æ•æ„Ÿå­—æ®µåˆ—è¡¨
    SENSITIVE_KEYS = {
        'OPENAI_API_KEY',
        'SILICONFLOW_API_KEY',
        'GEMINI_API_KEY',
        'API_KEY',
        'api_key',
        'password',
        'secret',
        'token',
        'authorization'
    }

    @staticmethod
    def sanitize_config(config: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºé…ç½®çš„å®‰å…¨å‰¯æœ¬ï¼Œéšè—æ•æ„Ÿä¿¡æ¯

        Args:
            config: åŸå§‹é…ç½®å­—å…¸

        Returns:
            è„±æ•åçš„é…ç½®å­—å…¸å‰¯æœ¬
        """
        safe_config = {}

        for key, value in config.items():
            if key in SecureLogger.SENSITIVE_KEYS:
                if value and isinstance(value, str):
                    # ä¿ç•™å‰8ä¸ªå­—ç¬¦ï¼Œå…¶ä½™ç”¨*æ›¿æ¢
                    safe_config[key] = value[:8] + "***" if len(value) > 8 else "***"
                else:
                    safe_config[key] = "***"
            elif isinstance(value, dict):
                # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
                safe_config[key] = SecureLogger.sanitize_config(value)
            else:
                safe_config[key] = value

        return safe_config

    @staticmethod
    def sanitize_string(text: str) -> str:
        """ä»å­—ç¬¦ä¸²ä¸­ç§»é™¤å¯èƒ½çš„APIå¯†é’¥

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            è„±æ•åçš„æ–‡æœ¬
        """
        # åŒ¹é…å¸¸è§çš„APIå¯†é’¥æ ¼å¼
        patterns = [
            r'sk-[a-zA-Z0-9]{48}',  # OpenAIæ ¼å¼
            r'[a-f0-9]{32}',         # é€šç”¨32ä½hex
            r'Bearer\s+[^\s]+',      # Bearer token
        ]

        result = text
        for pattern in patterns:
            result = re.sub(pattern, '***REDACTED***', result)

        return result

    @staticmethod
    def sanitize_error(error: Exception) -> str:
        """ä»å¼‚å¸¸æ¶ˆæ¯ä¸­ç§»é™¤æ•æ„Ÿä¿¡æ¯

        Args:
            error: å¼‚å¸¸å¯¹è±¡

        Returns:
            è„±æ•åçš„é”™è¯¯æ¶ˆæ¯
        """
        return SecureLogger.sanitize_string(str(error))
```

**Step 2: ä¿®æ”¹ ai_client.py ä½¿ç”¨å®‰å…¨æ—¥å¿—**

```python
# litrx/ai_client.py

from .security_utils import SecureLogger
from .logging_config import get_logger

logger = get_logger(__name__)

class AIClient:
    def __init__(self, config: Dict[str, Any]) -> None:
        service = config.get("AI_SERVICE", "openai")
        model = config.get("MODEL_NAME", "gpt-4o")

        # âœ… ä½¿ç”¨å®‰å…¨é…ç½®è®°å½•æ—¥å¿—
        logger.info(
            f"Initializing AIClient with service={service}, model={model}"
        )
        logger.debug(
            f"Configuration: {SecureLogger.sanitize_config(config)}"
        )

        # ... åˆå§‹åŒ–é€»è¾‘ ...

        logger.info("AIClient initialized successfully")

    def request(self, messages: List[Dict[str, Any]], **kwargs: Any) -> Dict[str, Any]:
        """å‘é€AIè¯·æ±‚"""
        try:
            logger.info(
                "Dispatching AI request | model=%s, messages=%d, temperature=%s",
                self.model,
                len(messages),
                kwargs.get("temperature", "<omitted>")
            )

            response = self.client.chat.completions.create(...)

            logger.info("AI request completed | usage=%s", getattr(response, 'usage', None))
            return response.model_dump()

        except Exception as e:
            # âœ… é”™è¯¯æ¶ˆæ¯ä¹Ÿè¦è„±æ•
            safe_error = SecureLogger.sanitize_error(e)
            logger.error(f"AI request failed: {safe_error}", exc_info=True)
            raise APIError(f"AIè¯·æ±‚å¤±è´¥: {safe_error}") from e
```

**Step 3: å…¨å±€å¼‚å¸¸é’©å­ï¼ˆå¯é€‰ä½†æ¨èï¼‰**

```python
# litrx/logging_config.py - æ·»åŠ 

import sys
from .security_utils import SecureLogger

def setup_exception_hook():
    """è®¾ç½®å…¨å±€å¼‚å¸¸é’©å­ï¼Œç¡®ä¿æ‰€æœ‰æœªæ•è·çš„å¼‚å¸¸éƒ½è¢«è„±æ•"""
    original_hook = sys.excepthook

    def secure_exception_hook(exc_type, exc_value, exc_traceback):
        """å®‰å…¨çš„å¼‚å¸¸é’©å­"""
        # è„±æ•å¼‚å¸¸æ¶ˆæ¯
        safe_message = SecureLogger.sanitize_error(exc_value)

        # æ›¿æ¢å¼‚å¸¸æ¶ˆæ¯
        if hasattr(exc_value, 'args') and exc_value.args:
            exc_value.args = (safe_message,) + exc_value.args[1:]

        # è°ƒç”¨åŸå§‹é’©å­
        original_hook(exc_type, exc_value, exc_traceback)

    sys.excepthook = secure_exception_hook

# åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
# litrx/__main__.py æˆ– run_gui.py
from litrx.logging_config import setup_exception_hook
setup_exception_hook()
```

**æµ‹è¯•æ¸…å•**:
```python
# tests/test_security_utils.py (æ–°å»º)

def test_sanitize_config():
    """æµ‹è¯•é…ç½®è„±æ•"""
    config = {
        "OPENAI_API_KEY": "sk-1234567890abcdefghijklmnopqrstuvwxyz",
        "MODEL_NAME": "gpt-4",
        "NESTED": {
            "API_KEY": "secret-key"
        }
    }

    safe = SecureLogger.sanitize_config(config)

    assert safe["OPENAI_API_KEY"] == "sk-12345***"
    assert safe["MODEL_NAME"] == "gpt-4"  # éæ•æ„Ÿå­—æ®µä¸å˜
    assert safe["NESTED"]["API_KEY"] == "***"

def test_sanitize_string():
    """æµ‹è¯•å­—ç¬¦ä¸²è„±æ•"""
    text = "Error: API key sk-1234567890abcdefghijklmnopqrstuvwxyz is invalid"
    safe = SecureLogger.sanitize_string(text)

    assert "sk-" not in safe
    assert "***REDACTED***" in safe

def test_sanitize_error():
    """æµ‹è¯•å¼‚å¸¸è„±æ•"""
    error = Exception("Invalid API key: sk-abcdefghijklmnop")
    safe = SecureLogger.sanitize_error(error)

    assert "sk-" not in safe
```

---

### 1.3 å¹¶å‘å®‰å…¨ä¿®å¤ (CRITICAL)

**é—®é¢˜æè¿°**:
- `df.iterrows()` åœ¨å¤šçº¿ç¨‹ä¸­ä¸å®‰å…¨
- ç¼ºå°‘workerå¼‚å¸¸å¤„ç†
- æ²¡æœ‰è¶…æ—¶æ§åˆ¶

**å½±å“èŒƒå›´**:
- `litrx/abstract_screener.py:653-675`

#### ğŸ“ è¯¦ç»†ä¿®æ”¹æ–¹æ¡ˆ

**Step 1: é¢„å¤„ç†DataFrameä¸ºçº¿ç¨‹å®‰å…¨çš„æ•°æ®ç»“æ„**

```python
# litrx/abstract_screener.py

from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from typing import List, Tuple, Dict, Any
import threading

class AbstractScreener:

    def batch_screen_parallel(
        self,
        df: pd.DataFrame,
        max_workers: int = 3,
        timeout_per_item: int = 120,
        stop_event: Optional[threading.Event] = None
    ) -> pd.DataFrame:
        """å¹¶å‘ç­›é€‰æ–‡çŒ®ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰

        Args:
            df: è¾“å…¥DataFrame
            max_workers: æœ€å¤§å¹¶å‘workeræ•°
            timeout_per_item: å•ä¸ªæ¡ç›®çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            stop_event: åœæ­¢ä¿¡å·

        Returns:
            å¤„ç†åçš„DataFrame
        """
        # âœ… Step 1: é¢„å…ˆè½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆé¿å…å¹¶å‘è®¿é—®è¿­ä»£å™¨ï¼‰
        rows_data: List[Tuple[int, Dict[str, Any]]] = [
            (idx, row.to_dict())
            for idx, row in df.iterrows()
        ]

        logger.info(f"Starting parallel screening: {len(rows_data)} items, {max_workers} workers")

        # ç»“æœå®¹å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        results_lock = threading.Lock()
        completed_count = 0
        failed_indices = []

        def process_single_article(index: int, row_dict: Dict[str, Any]) -> Tuple[int, Optional[Dict]]:
            """å¤„ç†å•ç¯‡æ–‡çŒ®ï¼ˆåœ¨workerçº¿ç¨‹ä¸­ï¼‰

            Returns:
                (index, results) æˆ– (index, None) å¦‚æœå¤±è´¥
            """
            try:
                # æ£€æŸ¥åœæ­¢ä¿¡å·
                if stop_event and stop_event.is_set():
                    logger.info(f"Worker for index {index} stopped by user")
                    return (index, None)

                # æå–æ•°æ®
                title = row_dict.get(self.title_col, "")
                abstract = row_dict.get(self.abstract_col, "")

                # è°ƒç”¨åˆ†æ
                results = self._analyze_single(title, abstract)

                logger.debug(f"Successfully processed index {index}")
                return (index, results)

            except Exception as e:
                logger.error(
                    f"Failed to process index {index}: {e}",
                    exc_info=True
                )
                return (index, None)

        # âœ… Step 2: ä½¿ç”¨ThreadPoolExecutor with timeout
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(process_single_article, idx, row_dict): idx
                for idx, row_dict in rows_data
            }

            # âœ… Step 3: å¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
            for future in as_completed(future_to_index, timeout=timeout_per_item * len(rows_data)):
                # æ£€æŸ¥åœæ­¢ä¿¡å·
                if stop_event and stop_event.is_set():
                    logger.info("Stop signal received, cancelling remaining tasks")
                    executor.shutdown(wait=False, cancel_futures=True)
                    break

                try:
                    # è·å–ç»“æœï¼ˆå¸¦è¶…æ—¶ï¼‰
                    index, results = future.result(timeout=timeout_per_item)

                    if results:
                        # âœ… çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°DataFrame
                        with results_lock:
                            self._apply_results_to_dataframe(df, index, results)
                            completed_count += 1
                    else:
                        with results_lock:
                            failed_indices.append(index)

                    # è¿›åº¦å›è°ƒ
                    if self.progress_callback:
                        with results_lock:
                            self.progress_callback(completed_count, len(rows_data), results)

                except TimeoutError:
                    index = future_to_index[future]
                    logger.error(f"Worker for index {index} timed out after {timeout_per_item}s")
                    with results_lock:
                        failed_indices.append(index)

                except Exception as e:
                    index = future_to_index[future]
                    logger.error(f"Worker for index {index} failed: {e}", exc_info=True)
                    with results_lock:
                        failed_indices.append(index)

        # âœ… Step 4: æŠ¥å‘Šç»“æœ
        logger.info(
            f"Parallel screening completed: "
            f"{completed_count} succeeded, {len(failed_indices)} failed"
        )

        if failed_indices:
            logger.warning(f"Failed indices: {failed_indices[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

        return df
```

**Step 2: æ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆå¯é€‰ï¼‰**

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class AbstractScreener:

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    def _analyze_single_with_retry(self, title: str, abstract: str) -> Dict[str, Any]:
        """å¸¦é‡è¯•çš„å•ç¯‡åˆ†æ

        è‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡ï¼Œç­‰å¾…æ—¶é—´æŒ‡æ•°å¢é•¿ï¼ˆ2s, 4s, 8sï¼‰
        """
        try:
            return self.analyzer.analyze(title, abstract)
        except (TimeoutError, ConnectionError) as e:
            logger.warning(f"Retryable error analyzing '{title[:50]}...': {e}")
            raise  # è®©tenacityé‡è¯•
        except Exception as e:
            logger.error(f"Non-retryable error: {e}")
            raise  # ä¸é‡è¯•ï¼Œç›´æ¥å¤±è´¥
```

**æµ‹è¯•æ¸…å•**:
```python
# tests/test_concurrent_screening.py (æ–°å»º)

import pytest
import pandas as pd
from litrx.abstract_screener import AbstractScreener
import threading

def test_parallel_screening_basic(mock_config):
    """æµ‹è¯•åŸºæœ¬å¹¶å‘ç­›é€‰"""
    screener = AbstractScreener(mock_config)

    df = pd.DataFrame({
        'Title': ['Paper 1', 'Paper 2', 'Paper 3'],
        'Abstract': ['Abstract 1', 'Abstract 2', 'Abstract 3']
    })

    result = screener.batch_screen_parallel(df, max_workers=2)

    assert len(result) == 3
    assert 'screening_result' in result.columns

def test_parallel_screening_with_stop(mock_config):
    """æµ‹è¯•åœæ­¢ä¿¡å·"""
    screener = AbstractScreener(mock_config)
    stop_event = threading.Event()

    def delayed_stop():
        time.sleep(1)
        stop_event.set()

    threading.Thread(target=delayed_stop, daemon=True).start()

    df = pd.DataFrame({'Title': ['P1'] * 100, 'Abstract': ['A1'] * 100})
    result = screener.batch_screen_parallel(df, stop_event=stop_event)

    # åº”è¯¥æå‰åœæ­¢ï¼Œä¸ä¼šå¤„ç†å…¨éƒ¨100æ¡
    assert len(result) < 100

def test_parallel_screening_timeout(mock_config, mock_slow_analyzer):
    """æµ‹è¯•è¶…æ—¶å¤„ç†"""
    screener = AbstractScreener(mock_config)
    screener.analyzer = mock_slow_analyzer  # æ¨¡æ‹Ÿæ…¢é€Ÿåˆ†æå™¨

    df = pd.DataFrame({'Title': ['P1'], 'Abstract': ['A1']})

    with pytest.raises(TimeoutError):
        screener.batch_screen_parallel(df, timeout_per_item=1)
```

---

### 1.4 GUIçº¿ç¨‹é˜»å¡ä¿®å¤ (CRITICAL)

**é—®é¢˜æè¿°**:
- é•¿æ—¶é—´æ“ä½œåœ¨ä¸»çº¿ç¨‹æ‰§è¡Œ
- UIå†»ç»“
- æ— æ³•å–æ¶ˆ

**å½±å“èŒƒå›´**:
- `litrx/gui/tabs_qt/*.py` æ‰€æœ‰æ ‡ç­¾é¡µ

#### ğŸ“ è¯¦ç»†ä¿®æ”¹æ–¹æ¡ˆ

**æ ‡å‡†æ¨¡å¼ï¼šåå°çº¿ç¨‹ + ä¿¡å·é€šçŸ¥**

```python
# litrx/gui/tabs_qt/csv_tab.py

from PyQt6.QtCore import QThread, pyqtSignal, QObject
from PyQt6.QtWidgets import QMessageBox
from ...task_manager import CancellableTask, TaskCancelledException
from ...exceptions import *

class AnalysisWorker(QObject):
    """åå°åˆ†æworker"""

    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int, int, str)  # (current, total, status)
    analysis_completed = pyqtSignal(object)  # (results_df)
    analysis_failed = pyqtSignal(str)  # (error_message)

    def __init__(self, analyzer, df, config):
        super().__init__()
        self.analyzer = analyzer
        self.df = df
        self.config = config
        self.task = CancellableTask()

    def run(self):
        """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ"""
        try:
            self.task.start()

            def progress_callback(current, total, result):
                """è¿›åº¦å›è°ƒï¼ˆåœ¨workerçº¿ç¨‹ä¸­ï¼‰"""
                if self.task.is_cancelled():
                    raise TaskCancelledException("ç”¨æˆ·å–æ¶ˆäº†åˆ†æ")

                status = f"æ­£åœ¨åˆ†æç¬¬ {current}/{total} ç¯‡..."
                self.progress_updated.emit(current, total, status)

            # æ‰§è¡Œåˆ†æ
            results = self.analyzer.batch_analyze(
                self.df,
                progress_callback=progress_callback,
                stop_event=self.task.cancelled
            )

            # å‘å°„å®Œæˆä¿¡å·
            self.analysis_completed.emit(results)

        except TaskCancelledException as e:
            logger.info(f"Analysis cancelled: {e}")
            self.analysis_failed.emit("åˆ†æå·²å–æ¶ˆ")

        except Exception as e:
            logger.error(f"Analysis failed: {e}", exc_info=True)
            self.analysis_failed.emit(str(e))

        finally:
            self.task.finish()

    def cancel(self):
        """å–æ¶ˆåˆ†æ"""
        self.task.cancel()


class CsvTab(QWidget):
    """CSVåˆ†ææ ‡ç­¾é¡µï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""

    def __init__(self, parent_window):
        super().__init__()
        self.parent_window = parent_window

        # åå°çº¿ç¨‹å’Œworker
        self.analysis_thread = None
        self.analysis_worker = None

        self._setup_ui()

    def start_analysis(self):
        """å¼€å§‹åˆ†æï¼ˆåœ¨åå°çº¿ç¨‹ï¼‰"""
        try:
            # 1. éªŒè¯è¾“å…¥
            if not self.df_file_path:
                QMessageBox.warning(self, "è­¦å‘Š", "è¯·å…ˆé€‰æ‹©CSVæ–‡ä»¶")
                return

            if not self.df or self.df.empty:
                QMessageBox.warning(self, "è­¦å‘Š", "CSVæ–‡ä»¶ä¸ºç©º")
                return

            # 2. ç¦ç”¨å¼€å§‹æŒ‰é’®ï¼Œå¯ç”¨åœæ­¢æŒ‰é’®
            self.start_btn.setEnabled(False)
            self.stop_btn.setEnabled(True)
            self.progress_bar.setValue(0)
            self.status_label.setText("å‡†å¤‡å¼€å§‹åˆ†æ...")

            # 3. åˆ›å»ºanalyzer
            config = self.parent_window.build_config()
            analyzer = LiteratureAnalyzer(config, research_topic=self.research_topic)

            # 4. åˆ›å»ºworkerå’Œçº¿ç¨‹
            self.analysis_worker = AnalysisWorker(analyzer, self.df, config)
            self.analysis_thread = QThread()

            # 5. ç§»åŠ¨workeråˆ°çº¿ç¨‹
            self.analysis_worker.moveToThread(self.analysis_thread)

            # 6. è¿æ¥ä¿¡å·
            self.analysis_thread.started.connect(self.analysis_worker.run)
            self.analysis_worker.progress_updated.connect(self._on_progress_updated)
            self.analysis_worker.analysis_completed.connect(self._on_analysis_completed)
            self.analysis_worker.analysis_failed.connect(self._on_analysis_failed)
            self.analysis_thread.finished.connect(self._on_thread_finished)

            # 7. å¯åŠ¨çº¿ç¨‹
            self.analysis_thread.start()

        except Exception as e:
            logger.error(f"Failed to start analysis: {e}", exc_info=True)
            QMessageBox.critical(
                self,
                "å¯åŠ¨å¤±è´¥",
                f"æ— æ³•å¯åŠ¨åˆ†æ:\n{str(e)}"
            )
            self._reset_ui()

    def stop_analysis(self):
        """åœæ­¢åˆ†æ"""
        if self.analysis_worker:
            reply = QMessageBox.question(
                self,
                "ç¡®è®¤åœæ­¢",
                "ç¡®å®šè¦åœæ­¢å½“å‰åˆ†æå—ï¼Ÿ\nå·²å®Œæˆçš„ç»“æœå°†ä¿ç•™ã€‚",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
            )

            if reply == QMessageBox.StandardButton.Yes:
                self.status_label.setText("æ­£åœ¨åœæ­¢...")
                self.analysis_worker.cancel()

    def _on_progress_updated(self, current: int, total: int, status: str):
        """å¤„ç†è¿›åº¦æ›´æ–°ï¼ˆåœ¨ä¸»çº¿ç¨‹ï¼‰"""
        self.progress_bar.setMaximum(total)
        self.progress_bar.setValue(current)
        self.status_label.setText(status)

    def _on_analysis_completed(self, results_df):
        """å¤„ç†åˆ†æå®Œæˆï¼ˆåœ¨ä¸»çº¿ç¨‹ï¼‰"""
        self.df = results_df
        self.status_label.setText(f"åˆ†æå®Œæˆï¼å…± {len(results_df)} æ¡ç»“æœ")
        self.progress_bar.setValue(self.progress_bar.maximum())

        # æ˜¾ç¤ºç»“æœ
        self._update_results_table(results_df)

        # è‡ªåŠ¨ä¿å­˜
        self._auto_save_results(results_df)

        QMessageBox.information(
            self,
            "å®Œæˆ",
            f"åˆ†æå®Œæˆï¼\nå…±å¤„ç† {len(results_df)} æ¡æ–‡çŒ®ã€‚"
        )

        self._reset_ui()

    def _on_analysis_failed(self, error_message: str):
        """å¤„ç†åˆ†æå¤±è´¥ï¼ˆåœ¨ä¸»çº¿ç¨‹ï¼‰"""
        self.status_label.setText("åˆ†æå¤±è´¥")

        QMessageBox.critical(
            self,
            "åˆ†æå¤±è´¥",
            f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:\n{error_message}\n\n"
            f"è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ï¼š\n"
            f"~/.litrx/logs/litrx.log"
        )

        self._reset_ui()

    def _on_thread_finished(self):
        """çº¿ç¨‹ç»“æŸæ¸…ç†ï¼ˆåœ¨ä¸»çº¿ç¨‹ï¼‰"""
        if self.analysis_thread:
            self.analysis_thread.quit()
            self.analysis_thread.wait()
            self.analysis_thread = None
        self.analysis_worker = None

    def _reset_ui(self):
        """é‡ç½®UIçŠ¶æ€"""
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)

    def closeEvent(self, event):
        """æ ‡ç­¾é¡µå…³é—­æ—¶æ¸…ç†"""
        if self.analysis_thread and self.analysis_thread.isRunning():
            reply = QMessageBox.question(
                self,
                "ç¡®è®¤å…³é—­",
                "åˆ†ææ­£åœ¨è¿›è¡Œä¸­ï¼Œç¡®å®šè¦å…³é—­å—ï¼Ÿ",
                QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
            )

            if reply == QMessageBox.StandardButton.No:
                event.ignore()
                return

            # åœæ­¢worker
            if self.analysis_worker:
                self.analysis_worker.cancel()

            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            if self.analysis_thread:
                self.analysis_thread.quit()
                self.analysis_thread.wait(3000)  # æœ€å¤šç­‰3ç§’

        event.accept()
```

**æµ‹è¯•æ¸…å•**:
```python
# tests/test_gui_threading.py (æ–°å»º)

import pytest
from PyQt6.QtWidgets import QApplication
from litrx.gui.tabs_qt.csv_tab import CsvTab
import time

@pytest.fixture
def app():
    """åˆ›å»ºQApplication"""
    app = QApplication([])
    yield app
    app.quit()

def test_analysis_in_background(app, qtbot, mock_config):
    """æµ‹è¯•åˆ†æåœ¨åå°çº¿ç¨‹è¿è¡Œ"""
    tab = CsvTab(mock_parent_window(mock_config))

    # åŠ è½½æµ‹è¯•æ•°æ®
    tab.df = pd.DataFrame({'Title': ['P1'], 'Abstract': ['A1']})
    tab.df_file_path = '/test/path.csv'

    # å¼€å§‹åˆ†æ
    tab.start_analysis()

    # éªŒè¯ï¼š
    # 1. å¼€å§‹æŒ‰é’®è¢«ç¦ç”¨
    assert not tab.start_btn.isEnabled()
    # 2. åœæ­¢æŒ‰é’®è¢«å¯ç”¨
    assert tab.stop_btn.isEnabled()
    # 3. ä¸»çº¿ç¨‹ä¸é˜»å¡ï¼ˆUIä»å¯å“åº”ï¼‰
    assert QApplication.instance().processEvents() >= 0

    # ç­‰å¾…å®Œæˆ
    qtbot.waitUntil(lambda: tab.start_btn.isEnabled(), timeout=10000)

def test_cancel_analysis(app, qtbot, mock_config):
    """æµ‹è¯•å–æ¶ˆåˆ†æ"""
    tab = CsvTab(mock_parent_window(mock_config))
    tab.df = pd.DataFrame({'Title': ['P1'] * 100, 'Abstract': ['A1'] * 100})
    tab.df_file_path = '/test/path.csv'

    # å¼€å§‹åˆ†æ
    tab.start_analysis()

    # ç­‰å¾…ä¸€ç‚¹æ—¶é—´
    qtbot.wait(1000)

    # å–æ¶ˆ
    tab.stop_analysis()

    # éªŒè¯UIæ¢å¤
    qtbot.waitUntil(lambda: tab.start_btn.isEnabled(), timeout=5000)
```

---

## ğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šæ¶æ„ä¼˜åŒ–ï¼ˆWeek 3-4ï¼‰

### 2.1 å•ä¸€èŒè´£åŸåˆ™é‡æ„ (HIGH)

**é—®é¢˜**: `LiteratureAnalyzer` ç±»æ‰¿æ‹…å¤ªå¤šèŒè´£

**ä¿®æ”¹æ–¹æ¡ˆ**: åˆ†ç¦»ä¸ºå¤šä¸ªä¸“æ³¨çš„ç±»

```python
# æ–°å»ºæ–‡ä»¶: litrx/data_loader.py
"""æ•°æ®åŠ è½½å’ŒéªŒè¯"""

from pathlib import Path
import pandas as pd
from .exceptions import FileProcessingError, ValidationError
from .utils import ColumnDetector

class DataLoader:
    """ä»…è´Ÿè´£æ•°æ®åŠ è½½å’ŒéªŒè¯"""

    def __init__(self, max_file_size_mb: int = 500):
        self.max_file_size_mb = max_file_size_mb

    def load_csv(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½CSVæ–‡ä»¶

        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„

        Returns:
            åŠ è½½çš„DataFrame

        Raises:
            FileProcessingError: æ–‡ä»¶è¯»å–å¤±è´¥
            ValidationError: æ–‡ä»¶éªŒè¯å¤±è´¥
        """
        path = Path(file_path)

        # 1. æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
        if not path.exists():
            raise FileProcessingError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # 2. æ–‡ä»¶å¤§å°æ£€æŸ¥
        file_size = path.stat().st_size
        max_bytes = self.max_file_size_mb * 1024 * 1024

        if file_size > max_bytes:
            raise FileProcessingError(
                f"æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB "
                f"(æœ€å¤§å…è®¸: {self.max_file_size_mb}MB)"
            )

        # 3. è¯»å–æ–‡ä»¶
        try:
            if file_size > 50 * 1024 * 1024:  # > 50MB
                # å¤§æ–‡ä»¶åˆ†å—è¯»å–
                chunks = pd.read_csv(
                    file_path,
                    encoding='utf-8-sig',
                    chunksize=10000
                )
                df = pd.concat(chunks, ignore_index=True)
            else:
                df = pd.read_csv(file_path, encoding='utf-8-sig')

        except pd.errors.ParserError as e:
            raise FileProcessingError(f"CSVæ ¼å¼é”™è¯¯: {str(e)}") from e
        except UnicodeDecodeError as e:
            raise FileProcessingError(f"æ–‡ä»¶ç¼–ç é”™è¯¯: {str(e)}") from e

        # 4. éªŒè¯æ•°æ®
        self._validate_dataframe(df, file_path)

        return df

    def _validate_dataframe(self, df: pd.DataFrame, file_path: str) -> None:
        """éªŒè¯DataFrameå†…å®¹

        Raises:
            ValidationError: éªŒè¯å¤±è´¥
        """
        if df.empty:
            raise ValidationError(f"æ–‡ä»¶ä¸ºç©º: {file_path}")

        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ['Title', 'Abstract']
        title_col = ColumnDetector.get_column(df, 'title')
        abstract_col = ColumnDetector.get_column(df, 'abstract')

        if not title_col:
            raise ValidationError(
                f"æœªæ‰¾åˆ°æ ‡é¢˜åˆ—ã€‚å¯ç”¨åˆ—: {', '.join(df.columns)}\n"
                f"æ”¯æŒçš„æ ‡é¢˜åˆ—å: Title, æ ‡é¢˜, Article Title"
            )

        if not abstract_col:
            raise ValidationError(
                f"æœªæ‰¾åˆ°æ‘˜è¦åˆ—ã€‚å¯ç”¨åˆ—: {', '.join(df.columns)}\n"
                f"æ”¯æŒçš„æ‘˜è¦åˆ—å: Abstract, æ‘˜è¦, Abstract Note"
            )

# æ–°å»ºæ–‡ä»¶: litrx/paper_analyzer.py
"""å•ç¯‡æ–‡çŒ®åˆ†æï¼ˆæ ¸å¿ƒAIé€»è¾‘ï¼‰"""

from typing import Dict, Any
from .ai_client import AIClient
from .cache import ResultCache

class PaperAnalyzer:
    """ä»…è´Ÿè´£å•ç¯‡æ–‡çŒ®çš„AIåˆ†æ"""

    def __init__(
        self,
        ai_client: AIClient,
        prompt_template: str,
        cache: ResultCache = None
    ):
        self.client = ai_client
        self.prompt_template = prompt_template
        self.cache = cache

        # ç»Ÿè®¡
        self.cache_hits = 0
        self.cache_misses = 0

    def analyze(
        self,
        title: str,
        abstract: str,
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """åˆ†æå•ç¯‡æ–‡çŒ®

        Args:
            title: è®ºæ–‡æ ‡é¢˜
            abstract: è®ºæ–‡æ‘˜è¦
            context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¦‚ç ”ç©¶ä¸»é¢˜ï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cache_key = self._build_cache_key(title, abstract, context)
            cached = self.cache.get(title, abstract, cache_key)

            if cached:
                self.cache_hits += 1
                logger.debug(f"Cache hit for '{title[:50]}...'")
                return cached

            self.cache_misses += 1

        # 2. æ„å»ºprompt
        prompt = self._build_prompt(title, abstract, context)

        # 3. è°ƒç”¨AI
        try:
            req_kwargs = {
                "messages": [{"role": "user", "content": prompt}]
            }

            if getattr(self.client, "supports_temperature", True):
                req_kwargs["temperature"] = 0.3

            response = self.client.request(**req_kwargs)
            content = response["choices"][0]["message"]["content"]

            # 4. è§£æç»“æœ
            result = self._parse_response(content)

            # 5. ç¼“å­˜ç»“æœ
            if self.cache:
                self.cache.set(title, abstract, result, cache_key)

            return result

        except Exception as e:
            logger.error(f"Analysis failed for '{title[:50]}...': {e}")
            raise

    def _build_cache_key(self, title: str, abstract: str, context: Dict) -> str:
        """æ„å»ºç¼“å­˜é”®"""
        import json
        context_str = json.dumps(context or {}, sort_keys=True)
        return f"{self.prompt_template[:50]}|{context_str}"

    def _build_prompt(self, title: str, abstract: str, context: Dict) -> str:
        """æ„å»ºåˆ†æprompt"""
        return self.prompt_template.format(
            title=title,
            abstract=abstract,
            **(context or {})
        )

    def _parse_response(self, content: str) -> Dict[str, Any]:
        """è§£æAIå“åº”"""
        from .utils import AIResponseParser
        return AIResponseParser.parse_relevance_response(content)

# ä¿®æ”¹åçš„ litrx/csv_analyzer.py
"""CSVç›¸å…³æ€§åˆ†æï¼ˆç»„åˆå™¨ï¼‰"""

from .data_loader import DataLoader
from .paper_analyzer import PaperAnalyzer
from .progress_manager import ProgressManager
from .cache import get_cache
from .ai_client import AIClient

class LiteratureAnalyzer:
    """æ–‡çŒ®åˆ†æå™¨ï¼ˆç»„åˆå¤šä¸ªä¸“æ³¨çš„ç»„ä»¶ï¼‰"""

    def __init__(
        self,
        config: Dict[str, Any],
        research_topic: str = "",
        questions: Dict[str, Any] = None
    ):
        self.research_topic = research_topic
        self.config = config
        self.questions = questions or {}

        # ç»„åˆä¾èµ–
        self.loader = DataLoader(
            max_file_size_mb=config.get("MAX_FILE_SIZE_MB", 500)
        )

        ai_client = AIClient(config)
        prompt_template = self._load_prompt_template()
        cache = get_cache() if config.get("ENABLE_CACHE", True) else None

        self.analyzer = PaperAnalyzer(ai_client, prompt_template, cache)

    def analyze_file(
        self,
        file_path: str,
        progress_callback: Callable = None
    ) -> pd.DataFrame:
        """åˆ†ææ•´ä¸ªCSVæ–‡ä»¶

        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒ

        Returns:
            åŒ…å«åˆ†æç»“æœçš„DataFrame
        """
        # 1. åŠ è½½æ•°æ®ï¼ˆå§”æ‰˜ç»™DataLoaderï¼‰
        df = self.loader.load_csv(file_path)

        # 2. æ‰¹é‡åˆ†æ
        results_df = self._batch_analyze(df, progress_callback)

        # 3. ä¿å­˜ç»“æœï¼ˆå§”æ‰˜ç»™ProgressManagerï¼‰
        output_path = self._generate_output_path(file_path)
        progress_mgr = ProgressManager(output_path)
        progress_mgr.finalize_results(results_df)

        return results_df

    def _batch_analyze(
        self,
        df: pd.DataFrame,
        progress_callback: Callable = None
    ) -> pd.DataFrame:
        """æ‰¹é‡åˆ†æï¼ˆä½¿ç”¨PaperAnalyzerï¼‰"""
        # æ·»åŠ ç»“æœåˆ—
        df['Relevance Score'] = None
        df['Analysis Result'] = None
        df['Literature Review Suggestion'] = None

        total = len(df)

        for idx, row in df.itertuples():
            # åˆ†æå•ç¯‡
            result = self.analyzer.analyze(
                title=row.Title,
                abstract=row.Abstract,
                context={"research_topic": self.research_topic}
            )

            # åº”ç”¨ç»“æœ
            df.at[idx, 'Relevance Score'] = result.get('relevance_score')
            df.at[idx, 'Analysis Result'] = result.get('analysis')
            df.at[idx, 'Literature Review Suggestion'] = result.get('literature_review_suggestion')

            # è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(idx + 1, total, result)

        return df
```

**å¥½å¤„**:
- æ¯ä¸ªç±»èŒè´£å•ä¸€ï¼Œæ˜“äºæµ‹è¯•
- ä¾èµ–æ³¨å…¥ï¼Œæ˜“äºmock
- å¯ä»¥ç‹¬ç«‹æ¼”è¿›æ¯ä¸ªç»„ä»¶

---

### 2.2 ä»£ç é‡å¤æ¶ˆé™¤ (MEDIUM)

**é—®é¢˜**: é…ç½®åŠ è½½é€»è¾‘åœ¨å¤šä¸ªæ¨¡å—é‡å¤

**ä¿®æ”¹æ–¹æ¡ˆ**: åˆ›å»ºç»Ÿä¸€çš„é…ç½®å·¥å‚

```python
# litrx/config_factory.py (æ–°å»º)
"""ç»Ÿä¸€çš„é…ç½®åŠ è½½å·¥å‚"""

from typing import Tuple, Dict, Any, Optional
from pathlib import Path
import json
import yaml

from .config import load_config as base_load_config, DEFAULT_CONFIG
from .resources import resource_path

class ConfigFactory:
    """é…ç½®å·¥å‚ï¼šç»Ÿä¸€åŠ è½½å„æ¨¡å—çš„é…ç½®"""

    @staticmethod
    def load_for_csv(
        config_path: Optional[str] = None,
        questions_path: Optional[str] = None
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """åŠ è½½CSVåˆ†ææ¨¡å—é…ç½®

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            questions_path: é—®é¢˜æ¨¡æ¿è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            (config, questions) å…ƒç»„
        """
        # åŠ è½½åŸºç¡€é…ç½®
        default_cfg = resource_path("configs", "config.yaml")
        config = base_load_config(str(config_path or default_cfg), DEFAULT_CONFIG)

        # åŠ è½½é—®é¢˜æ¨¡æ¿
        q_path = questions_path or resource_path("configs", "questions", "csv.yaml")

        if q_path.exists():
            with open(q_path, 'r', encoding='utf-8') as f:
                questions = yaml.safe_load(f) or {}
        else:
            questions = {}

        return config, questions

    @staticmethod
    def load_for_abstract(
        config_path: Optional[str] = None,
        mode: Optional[str] = None
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """åŠ è½½æ‘˜è¦ç­›é€‰æ¨¡å—é…ç½®

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            mode: ç­›é€‰æ¨¡å¼åç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            (config, questions) å…ƒç»„
        """
        # åŠ è½½åŸºç¡€é…ç½®
        default_cfg = resource_path("configs", "config.yaml")
        config = base_load_config(str(config_path or default_cfg), DEFAULT_CONFIG)

        # åŠ è½½æ¨¡å¼é—®é¢˜
        questions = ConfigFactory._load_mode_questions(mode or config.get("CONFIG_MODE", "default"))

        return config, questions

    @staticmethod
    def load_for_matrix(
        config_path: Optional[str] = None,
        dimensions_file: Optional[str] = None
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """åŠ è½½æ–‡çŒ®çŸ©é˜µæ¨¡å—é…ç½®

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            dimensions_file: ç»´åº¦é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰

        Returns:
            (config, dimensions) å…ƒç»„
        """
        # åŠ è½½åŸºç¡€é…ç½®
        default_cfg = resource_path("configs", "config.yaml")
        config = base_load_config(str(config_path or default_cfg), DEFAULT_CONFIG)

        # åŠ è½½ç»´åº¦é…ç½®
        dim_path = dimensions_file or resource_path("configs", "matrix", "default.yaml")

        if Path(dim_path).exists():
            with open(dim_path, 'r', encoding='utf-8') as f:
                dimensions = yaml.safe_load(f) or {}
        else:
            dimensions = {"dimensions": []}

        return config, dimensions

    @staticmethod
    def _load_mode_questions(mode: str) -> Dict[str, Any]:
        """åŠ è½½ç­›é€‰æ¨¡å¼é—®é¢˜ï¼ˆç§æœ‰æ–¹æ³•ï¼‰"""
        # å°è¯•æ–°æ ¼å¼
        unified_path = resource_path("configs", "abstract", f"{mode}.yaml")

        if unified_path.exists():
            with open(unified_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)

        # å›é€€åˆ°æ—§æ ¼å¼
        legacy_path = resource_path("questions_config.json")

        if legacy_path.exists():
            with open(legacy_path, 'r', encoding='utf-8') as f:
                all_modes = json.load(f)
            return all_modes.get(mode, {
                "open_questions": [],
                "yes_no_questions": []
            })

        return {"open_questions": [], "yes_no_questions": []}

# ä½¿ç”¨ç¤ºä¾‹ï¼š
# åœ¨ csv_analyzer.py ä¸­ï¼š
from .config_factory import ConfigFactory

config, questions = ConfigFactory.load_for_csv()
analyzer = LiteratureAnalyzer(config, questions=questions)

# åœ¨ abstract_screener.py ä¸­ï¼š
config, questions = ConfigFactory.load_for_abstract(mode="psychology_empirical")
screener = AbstractScreener(config, questions)

# åœ¨ matrix_analyzer.py ä¸­ï¼š
config, dimensions = ConfigFactory.load_for_matrix(dimensions_file="custom.yaml")
matrix = MatrixAnalyzer(config, dimensions)
```

---

## ğŸ“ˆ æµ‹è¯•è¦†ç›–ç‡æå‡è®¡åˆ’ï¼ˆWeek 5-6ï¼‰

ç›®æ ‡ï¼šä» 15% â†’ 85%

### 3.1 å…³é”®æ¨¡å—æµ‹è¯•ä¼˜å…ˆçº§

**Priority 1 (Week 5)**:
```
âœ… ai_client.py         - ç›®æ ‡ 90% è¦†ç›–
âœ… cache.py            - ç›®æ ‡ 85% è¦†ç›–
âœ… progress_manager.py - ç›®æ ‡ 80% è¦†ç›–
âœ… exceptions.py       - ç›®æ ‡ 95% è¦†ç›– (ç®€å•æ¨¡å—)
```

**Priority 2 (Week 6)**:
```
âœ… csv_analyzer.py     - ç›®æ ‡ 75% è¦†ç›–
âœ… abstract_screener.py - ç›®æ ‡ 70% è¦†ç›–
âœ… matrix_analyzer.py  - ç›®æ ‡ 70% è¦†ç›–
```

**Priority 3 (æŒç»­)**:
```
â–¡ GUI components      - ç›®æ ‡ 60% è¦†ç›–
â–¡ Utilities           - ç›®æ ‡ 85% è¦†ç›–
```

### 3.2 æµ‹è¯•æ–‡ä»¶ç»“æ„

```
tests/
â”œâ”€â”€ unit/                      # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_ai_client.py      # AIå®¢æˆ·ç«¯æµ‹è¯•
â”‚   â”œâ”€â”€ test_cache.py          # ç¼“å­˜ç³»ç»Ÿæµ‹è¯•
â”‚   â”œâ”€â”€ test_progress_manager.py # è¿›åº¦ç®¡ç†æµ‹è¯•
â”‚   â”œâ”€â”€ test_exceptions.py     # å¼‚å¸¸ç±»æµ‹è¯•
â”‚   â”œâ”€â”€ test_data_loader.py    # æ•°æ®åŠ è½½æµ‹è¯•
â”‚   â”œâ”€â”€ test_paper_analyzer.py # æ–‡çŒ®åˆ†ææµ‹è¯•
â”‚   â””â”€â”€ test_security_utils.py # å®‰å…¨å·¥å…·æµ‹è¯•
â”‚
â”œâ”€â”€ integration/               # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_csv_workflow.py  # CSVå®Œæ•´æµç¨‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_abstract_workflow.py # æ‘˜è¦ç­›é€‰æµç¨‹æµ‹è¯•
â”‚   â””â”€â”€ test_matrix_workflow.py # çŸ©é˜µåˆ†ææµç¨‹æµ‹è¯•
â”‚
â”œâ”€â”€ gui/                       # GUIæµ‹è¯•
â”‚   â”œâ”€â”€ test_csv_tab.py
â”‚   â”œâ”€â”€ test_abstract_tab.py
â”‚   â””â”€â”€ test_matrix_tab.py
â”‚
â”œâ”€â”€ fixtures/                  # æµ‹è¯•fixtures
â”‚   â”œâ”€â”€ mock_data.py           # Mockæ•°æ®ç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ mock_ai_client.py      # Mock AIå®¢æˆ·ç«¯
â”‚   â””â”€â”€ sample_files/          # ç¤ºä¾‹æ–‡ä»¶
â”‚       â”œâ”€â”€ sample.csv
â”‚       â”œâ”€â”€ sample.xlsx
â”‚       â””â”€â”€ sample_config.yaml
â”‚
â””â”€â”€ conftest.py                # pytesté…ç½®
```

### 3.3 ç¤ºä¾‹æµ‹è¯•æ–‡ä»¶

```python
# tests/unit/test_ai_client.py
"""AIå®¢æˆ·ç«¯å•å…ƒæµ‹è¯•"""

import pytest
from unittest.mock import Mock, patch
from litrx.ai_client import AIClient
from litrx.exceptions import APIError

@pytest.fixture
def mock_config():
    """åŸºç¡€é…ç½®fixture"""
    return {
        "AI_SERVICE": "openai",
        "MODEL_NAME": "gpt-4",
        "OPENAI_API_KEY": "sk-test1234567890",
        "AI_TIMEOUT_SECONDS": 60
    }

@pytest.fixture
def mock_openai_client():
    """Mock OpenAIå®¢æˆ·ç«¯"""
    with patch('litrx.ai_client.OpenAI') as mock:
        yield mock

class TestAIClientInit:
    """æµ‹è¯•AIClientåˆå§‹åŒ–"""

    def test_init_with_openai(self, mock_config, mock_openai_client):
        """æµ‹è¯•OpenAIæœåŠ¡åˆå§‹åŒ–"""
        client = AIClient(mock_config)

        assert client.service == "openai"
        assert client.model == "gpt-4"
        assert mock_openai_client.called

    def test_init_with_siliconflow(self, mock_config, mock_openai_client):
        """æµ‹è¯•SiliconFlowæœåŠ¡åˆå§‹åŒ–"""
        mock_config["AI_SERVICE"] = "siliconflow"
        mock_config["SILICONFLOW_API_KEY"] = "sf-test1234"

        client = AIClient(mock_config)

        assert client.service == "siliconflow"
        # éªŒè¯ä½¿ç”¨äº†æ­£ç¡®çš„base_url
        mock_openai_client.assert_called_with(
            api_key="sf-test1234",
            base_url="https://api.siliconflow.cn/v1"
        )

    def test_init_missing_api_key(self, mock_config):
        """æµ‹è¯•ç¼ºå°‘APIå¯†é’¥æ—¶æŠ›å‡ºå¼‚å¸¸"""
        del mock_config["OPENAI_API_KEY"]

        with pytest.raises(RuntimeError, match="API key not configured"):
            AIClient(mock_config)

class TestAIClientRequest:
    """æµ‹è¯•AIClientè¯·æ±‚æ–¹æ³•"""

    def test_request_success(self, mock_config, mock_openai_client):
        """æµ‹è¯•æˆåŠŸçš„è¯·æ±‚"""
        # è®¾ç½®mockè¿”å›å€¼
        mock_response = Mock()
        mock_response.model_dump.return_value = {
            "choices": [{
                "message": {"content": "Test response"}
            }],
            "usage": {"total_tokens": 100}
        }

        mock_instance = Mock()
        mock_instance.chat.completions.create.return_value = mock_response
        mock_openai_client.return_value = mock_instance

        # æ‰§è¡Œæµ‹è¯•
        client = AIClient(mock_config)
        result = client.request(
            messages=[{"role": "user", "content": "Test"}],
            temperature=0.5
        )

        # éªŒè¯
        assert result["choices"][0]["message"]["content"] == "Test response"
        assert result["usage"]["total_tokens"] == 100

    def test_request_with_unsupported_temperature(self, mock_config, mock_openai_client):
        """æµ‹è¯•æ¨¡å‹ä¸æ”¯æŒtemperatureå‚æ•°æ—¶çš„é‡è¯•"""
        # ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥ï¼ˆtemperatureä¸æ”¯æŒï¼‰
        # ç¬¬äºŒæ¬¡è°ƒç”¨æˆåŠŸï¼ˆæ— temperatureï¼‰

        mock_instance = Mock()

        # ç¬¬ä¸€æ¬¡è°ƒç”¨æŠ›å¼‚å¸¸
        mock_instance.chat.completions.create.side_effect = [
            Exception("Unsupported value: param 'temperature'"),
            Mock(model_dump=lambda: {"choices": [{"message": {"content": "OK"}}]})
        ]

        mock_openai_client.return_value = mock_instance

        client = AIClient(mock_config)
        result = client.request(
            messages=[{"role": "user", "content": "Test"}],
            temperature=0.5
        )

        # éªŒè¯æˆåŠŸé‡è¯•
        assert result["choices"][0]["message"]["content"] == "OK"
        # éªŒè¯è°ƒç”¨äº†ä¸¤æ¬¡ï¼ˆç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œç¬¬äºŒæ¬¡æˆåŠŸï¼‰
        assert mock_instance.chat.completions.create.call_count == 2

    def test_request_timeout(self, mock_config, mock_openai_client):
        """æµ‹è¯•è¯·æ±‚è¶…æ—¶"""
        mock_instance = Mock()
        mock_instance.chat.completions.create.side_effect = TimeoutError("Request timeout")
        mock_openai_client.return_value = mock_instance

        client = AIClient(mock_config)

        with pytest.raises(APIError, match="AIè¯·æ±‚å¤±è´¥"):
            client.request(messages=[{"role": "user", "content": "Test"}])

# tests/integration/test_csv_workflow.py
"""CSVåˆ†æå®Œæ•´æµç¨‹é›†æˆæµ‹è¯•"""

import pytest
import pandas as pd
from pathlib import Path
import tempfile

from litrx.csv_analyzer import LiteratureAnalyzer
from litrx.exceptions import FileProcessingError

@pytest.fixture
def sample_csv(tmp_path):
    """åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶"""
    csv_path = tmp_path / "sample.csv"

    df = pd.DataFrame({
        'Title': [
            'Machine Learning in Healthcare',
            'Deep Learning Applications',
            'Natural Language Processing'
        ],
        'Abstract': [
            'This paper discusses the use of ML in healthcare...',
            'We present a novel deep learning architecture...',
            'NLP techniques for text analysis...'
        ]
    })

    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    return csv_path

@pytest.fixture
def mock_ai_config():
    """æµ‹è¯•ç”¨AIé…ç½®ï¼ˆä½¿ç”¨mockï¼‰"""
    return {
        "AI_SERVICE": "openai",
        "MODEL_NAME": "gpt-4",
        "OPENAI_API_KEY": "sk-test",
        "ENABLE_CACHE": False  # æµ‹è¯•æ—¶ç¦ç”¨ç¼“å­˜
    }

def test_end_to_end_csv_analysis(sample_csv, mock_ai_config, mocker):
    """æµ‹è¯•å®Œæ•´çš„CSVåˆ†ææµç¨‹"""
    # Mock AIClientè¿”å›
    mock_response = {
        "choices": [{
            "message": {
                "content": '''
                {
                    "relevance_score": 85,
                    "analysis": "Highly relevant to ML research",
                    "literature_review_suggestion": "Should be included"
                }
                '''
            }
        }]
    }

    mocker.patch(
        'litrx.ai_client.AIClient.request',
        return_value=mock_response
    )

    # åˆ›å»ºåˆ†æå™¨
    analyzer = LiteratureAnalyzer(
        config=mock_ai_config,
        research_topic="Machine Learning Applications"
    )

    # æ‰§è¡Œåˆ†æ
    results = analyzer.analyze_file(str(sample_csv))

    # éªŒè¯ç»“æœ
    assert len(results) == 3
    assert 'Relevance Score' in results.columns
    assert 'Analysis Result' in results.columns
    assert results['Relevance Score'].iloc[0] == 85

def test_csv_analysis_with_invalid_file(mock_ai_config):
    """æµ‹è¯•æ— æ•ˆæ–‡ä»¶å¤„ç†"""
    analyzer = LiteratureAnalyzer(mock_ai_config)

    with pytest.raises(FileProcessingError, match="æ–‡ä»¶ä¸å­˜åœ¨"):
        analyzer.analyze_file("/nonexistent/file.csv")
```

### 3.4 CI/CDé›†æˆ

```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-cov pytest-qt pytest-mock

    - name: Run tests with coverage
      run: |
        pytest --cov=litrx --cov-report=xml --cov-report=html tests/

    - name: Upload coverage
      uses: codecov/codecov-action@v2
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

    - name: Check coverage threshold
      run: |
        # è¦æ±‚æœ€ä½80%è¦†ç›–ç‡
        coverage report --fail-under=80
```

---

## ğŸ“š æ–‡æ¡£æ”¹è¿›ï¼ˆè´¯ç©¿æ•´ä¸ªè¿‡ç¨‹ï¼‰

### 4.1 ä»£ç æ–‡æ¡£æ ‡å‡†

**æ‰€æœ‰å…¬å…±å‡½æ•°å¿…é¡»æœ‰docstring**:

```python
def analyze_paper(
    self,
    title: str,
    abstract: str,
    context: Dict[str, Any] = None
) -> Dict[str, Any]:
    """åˆ†æå•ç¯‡æ–‡çŒ®çš„ç›¸å…³æ€§ã€‚

    ä½¿ç”¨AIæ¨¡å‹è¯„ä¼°æ–‡çŒ®ä¸ç ”ç©¶ä¸»é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

    Args:
        title: æ–‡çŒ®æ ‡é¢˜ï¼Œç”¨äºåˆæ­¥åˆ¤æ–­ç›¸å…³æ€§
        abstract: æ–‡çŒ®æ‘˜è¦ï¼Œä¸»è¦çš„åˆ†ææ¥æº
        context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
            - research_topic (str): ç ”ç©¶ä¸»é¢˜
            - additional_criteria (List[str], optional): é¢å¤–ç­›é€‰æ ‡å‡†

    Returns:
        åˆ†æç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
        - relevance_score (int): ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
        - analysis (str): è¯¦ç»†åˆ†æè¯´æ˜
        - literature_review_suggestion (str): æ–‡çŒ®ç»¼è¿°å»ºè®®
        - keywords (List[str]): æå–çš„å…³é”®è¯

    Raises:
        APIError: AIæœåŠ¡è°ƒç”¨å¤±è´¥
        ValidationError: è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥

    Examples:
        >>> analyzer = PaperAnalyzer(client, template)
        >>> result = analyzer.analyze(
        ...     title="Machine Learning in Healthcare",
        ...     abstract="This paper presents...",
        ...     context={"research_topic": "AI in Medicine"}
        ... )
        >>> print(result['relevance_score'])
        85

    Note:
        - ç»“æœä¼šè‡ªåŠ¨ç¼“å­˜ï¼Œç›¸åŒè¾“å…¥ä¼šç›´æ¥è¿”å›ç¼“å­˜
        - åˆ†ææ—¶é—´å–å†³äºAIæ¨¡å‹å“åº”é€Ÿåº¦ï¼ˆé€šå¸¸5-15ç§’ï¼‰
        - åˆ†æ•°ç®—æ³•åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…

    See Also:
        - :func:`batch_analyze`: æ‰¹é‡åˆ†æå¤šç¯‡æ–‡çŒ®
        - :class:`AIClient`: AIå®¢æˆ·ç«¯é…ç½®
    """
    pass
```

### 4.2 CHANGELOGç»´æŠ¤

```markdown
# CHANGELOG.md

## [Unreleased]

### Added
- å®‰å…¨æ—¥å¿—å·¥å…·ï¼ˆSecureLoggerï¼‰é˜²æ­¢APIå¯†é’¥æ³„éœ²
- å®Œæ•´çš„å¼‚å¸¸å±‚æ¬¡ç»“æ„ï¼ˆLitRxErroråŠå­ç±»ï¼‰
- ConfigFactoryç»Ÿä¸€é…ç½®åŠ è½½
- æ•°æ®åŠ è½½å™¨ï¼ˆDataLoaderï¼‰ç‹¬ç«‹æ¨¡å—
- æ–‡çŒ®åˆ†æå™¨ï¼ˆPaperAnalyzerï¼‰ç‹¬ç«‹æ¨¡å—

### Changed
- **BREAKING**: LiteratureAnalyzerç°åœ¨ä½¿ç”¨ç»„åˆè€Œéç»§æ‰¿
- æ‰€æœ‰GUIæ“ä½œæ”¹ä¸ºåå°çº¿ç¨‹æ‰§è¡Œ
- å¼‚å¸¸å¤„ç†ä½¿ç”¨è‡ªå®šä¹‰å¼‚å¸¸ç±»è€Œéæ³›åŒ–Exception
- å¹¶å‘ç­›é€‰ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ•°æ®ç»“æ„

### Fixed
- ä¿®å¤GUIçº¿ç¨‹é˜»å¡å¯¼è‡´"æ— å“åº”"é—®é¢˜
- ä¿®å¤å¹¶å‘åœºæ™¯ä¸‹DataFrameè¿­ä»£å™¨ä¸å®‰å…¨
- ä¿®å¤åº“ä»£ç ä¸­ä½¿ç”¨sys.exit()å¯¼è‡´ç¨‹åºå´©æºƒ
- ä¿®å¤APIå¯†é’¥å¯èƒ½åœ¨æ—¥å¿—ä¸­æ³„éœ²çš„å®‰å…¨é—®é¢˜

### Security
- APIå¯†é’¥åœ¨æ‰€æœ‰æ—¥å¿—è¾“å‡ºä¸­è‡ªåŠ¨è„±æ•
- æ·»åŠ å…¨å±€å¼‚å¸¸é’©å­ç¡®ä¿å¼‚å¸¸æ¶ˆæ¯å®‰å…¨
- æ–‡ä»¶è·¯å¾„éªŒè¯é˜²æ­¢è·¯å¾„éå†æ”»å‡»

## [0.2.0] - 2024-11-19

### Added
- AIè¾…åŠ©é…ç½®ç”ŸæˆåŠŸèƒ½
- ç»“æœç¼“å­˜ç³»ç»Ÿ
- è¿›åº¦ç®¡ç†ç³»ç»Ÿ
- å®‰å…¨å¯†é’¥ç®¡ç†

...
```

---

## âœ… éªŒæ”¶æ ‡å‡†

### é˜¶æ®µ1å®Œæˆæ ‡å‡†ï¼ˆWeek 2ç»“æŸï¼‰:
- [ ] æ— ä»»ä½•`sys.exit()`åœ¨åº“ä»£ç ä¸­
- [ ] æ‰€æœ‰å¼‚å¸¸ä½¿ç”¨è‡ªå®šä¹‰å¼‚å¸¸ç±»
- [ ] APIå¯†é’¥ä¸å‡ºç°åœ¨æ—¥å¿—ä¸­ï¼ˆé€šè¿‡æµ‹è¯•éªŒè¯ï¼‰
- [ ] GUIä¸ä¼šåœ¨é•¿æ“ä½œæ—¶å†»ç»“ï¼ˆé€šè¿‡æ‰‹åŠ¨æµ‹è¯•ï¼‰
- [ ] å¹¶å‘ç­›é€‰é€šè¿‡100+æ¡ç›®çš„å‹åŠ›æµ‹è¯•

### é˜¶æ®µ2å®Œæˆæ ‡å‡†ï¼ˆWeek 4ç»“æŸï¼‰:
- [ ] LiteratureAnalyzeråˆ†ç¦»ä¸º3ä¸ªç‹¬ç«‹ç±»
- [ ] é…ç½®åŠ è½½ç»Ÿä¸€ä½¿ç”¨ConfigFactory
- [ ] ä»£ç é‡å¤åº¦ä»24% â†’ <10%
- [ ] æ‰€æœ‰æ–°ä»£ç æœ‰å•å…ƒæµ‹è¯•

### æœ€ç»ˆéªŒæ”¶æ ‡å‡†ï¼ˆWeek 6ç»“æŸï¼‰:
- [ ] æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°85%
- [ ] æ‰€æœ‰CRITICALå’ŒHIGHé—®é¢˜å·²ä¿®å¤
- [ ] CI/CDæµæ°´çº¿è¿è¡Œé€šè¿‡
- [ ] æ–‡æ¡£æ›´æ–°åˆ°æœ€æ–°
- [ ] ä»£ç è´¨é‡è¯„åˆ†ä»2.3/5 â†’ 4.2/5

---

## ğŸ“… è¯¦ç»†æ—¶é—´è¡¨

### Week 1
**Monday-Tuesday**: å¼‚å¸¸å¤„ç†é‡æ„
- åˆ›å»ºexceptions.pyå®Œæ•´å±‚æ¬¡
- ä¿®æ”¹csv_analyzer.py
- ä¿®æ”¹abstract_screener.py
- ç¼–å†™å¼‚å¸¸æµ‹è¯•

**Wednesday-Thursday**: APIå¯†é’¥å®‰å…¨
- åˆ›å»ºsecurity_utils.py
- ä¿®æ”¹ai_client.py
- æ·»åŠ å…¨å±€å¼‚å¸¸é’©å­
- ç¼–å†™å®‰å…¨æµ‹è¯•

**Friday**: å¹¶å‘å®‰å…¨ä¿®å¤
- é‡æ„abstract_screenerå¹¶å‘é€»è¾‘
- æ·»åŠ è¶…æ—¶å’Œå¼‚å¸¸å¤„ç†
- ç¼–å†™å¹¶å‘æµ‹è¯•

### Week 2
**Monday-Tuesday**: GUIçº¿ç¨‹ä¿®å¤
- é‡æ„csv_tab.py
- é‡æ„abstract_tab.py
- é‡æ„matrix_tab.py
- æ·»åŠ QThreadç¤ºä¾‹

**Wednesday-Thursday**: é›†æˆæµ‹è¯•
- æµ‹è¯•æ‰€æœ‰ä¿®å¤
- ä¿®å¤å‘ç°çš„é—®é¢˜
- æ€§èƒ½æµ‹è¯•

**Friday**: ä»£ç å®¡æŸ¥å’Œæ–‡æ¡£
- å›¢é˜Ÿä»£ç å®¡æŸ¥
- æ›´æ–°CHANGELOG
- æ›´æ–°README

### Week 3-4
**æ¶æ„ä¼˜åŒ–** (æŒ‰è®¡åˆ’æ‰§è¡Œ)

### Week 5-6
**æµ‹è¯•è¦†ç›–ç‡æå‡** (æŒ‰è®¡åˆ’æ‰§è¡Œ)

---

## ğŸ”„ æŒç»­æ”¹è¿›

### æ¯å‘¨æ£€æŸ¥æ¸…å•
```markdown
- [ ] è¿è¡Œå…¨éƒ¨æµ‹è¯•å¥—ä»¶
- [ ] æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡è¶‹åŠ¿
- [ ] ä»£ç å®¡æŸ¥æ–°å¢ä»£ç 
- [ ] æ›´æ–°æ–‡æ¡£ï¼ˆå¦‚æœ‰APIå˜æ›´ï¼‰
- [ ] æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
- [ ] å®¡æŸ¥æ–°å¢ä¾èµ–
```

### æ¯æœˆå®¡è®¡
```markdown
- [ ] ä»£ç è´¨é‡è¯„åˆ†
- [ ] æŠ€æœ¯å€ºåŠ¡è¯„ä¼°
- [ ] å®‰å…¨æ¼æ´æ‰«æ
- [ ] ä¾èµ–ç‰ˆæœ¬æ›´æ–°
- [ ] æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-11-19
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
**ä¸‹æ¬¡å®¡æŸ¥**: 2025-12-03 (Week 2ç»“æŸ)
